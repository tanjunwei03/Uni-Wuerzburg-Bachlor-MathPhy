\documentclass[prb,12pt]{revtex4-2}

\usepackage{amsmath, amssymb,physics,amsfonts,amsthm}
\usepackage{enumitem}
\usepackage[most]{tcolorbox}
\usepackage{cancel}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{xurl}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[normalem]{ulem}
\usepackage{transparent}
\usepackage{float}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage[framemethod=TikZ]{mdframed}
\mdfsetup{skipabove=1em,skipbelow=0em, innertopmargin=12pt, innerbottommargin=8pt}
\declaretheoremstyle[headfont=\bfseries\sffamily, bodyfont=\normalfont, mdframed={  } ]{thmbox}
%put nobreak in mdframed
\declaretheorem[style=thmbox, name=Theorem]{Theorem}
\declaretheorem[sibling=Theorem, style=thmbox, name=Definition]{Definition}
\declaretheorem[sibling=Theorem, style=thmbox, name=Corollary]{Corollary}
\declaretheorem[sibling=Theorem, style=thmbox, name=Example]{Example}
\declaretheorem[sibling=Theorem, style=thmbox, name=Proposition]{Proposition}
\declaretheorem[sibling=Theorem, style=thmbox, name=Lemma]{Lemma}
%\newtheorem{Theorem}{Theorem}
%\numberwithin{Theorem}{chapter}
%\newtheorem{Proposition}{Proposition}
%\newtheorem{Lemma}[Theorem]{Lemma}
%\newtheorem{Corollary}[Theorem]{Corollary}
%\newtheorem{Example}[Theorem]{Example}
%\theoremstyle{definition}
\theoremstyle{definition}
\newtheorem{Remark}[Theorem]{Remark}
\theoremstyle{definition}
\newtheorem{Problem}{Problem}
\theoremstyle{definition}
\newenvironment{parts}{\begin{enumerate}[label=(\alph*)]}{\end{enumerate}}
%tikz	
\tcbset{breakable=true,toprule at break = 0mm,bottomrule at break = 0mm}
\usetikzlibrary{patterns}
\usetikzlibrary{matrix}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
% definitions of number sets
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\allowdisplaybreaks
\begin{document}
	\title{Stochastic Differential Equations}
	\author{Jun Wei Tan}
	\email{jun-wei.tan@stud-mail.uni-wuerzburg.de}
	\affiliation{Julius-Maximilians-Universit\"{a}t W\"{u}rzburg}
	\date{\today}
	\maketitle

\begin{Definition}
	A stochastic differential equation is a (formal) equation of the form
	\begin{equation}\label{eq:SDEdiff}
	\dv{X_t}{t}=b(t, X_t)+\sigma(t, X_t)W_t,
\end{equation}
	where $W_t$ is white noise. 
\end{Definition}
This equation is to be interpreted as follows:
\begin{Definition}
	We say that the stochastic process $X_t$ is a \emph{solution} of the SDE \eqref{eq:SDEdiff} if
	\[X_t = X_0+\int_0^t b(s, X_s)\dd{s}+ \int_0^t \sigma(s, X_s)\dd{B_s}\]
\end{Definition}
The standard method to solve a stochastic differential equation is the Itô formula
\begin{Theorem}[The 1-Dimensional Itô Formula]
	Suppose $X_t$ is an Itô process defined by the formula
	\[\dd{X_t}=u\dd{t}+v\dd{B_t}.\]
	Let $g(t,x)\in C^2([0,\infty)\times \R)$. Then
	\[Y_t=g(t, X_t)\]
	is also an Itô process, and
	\[\dd{Y_t}=\pdv{g}{t}(t, X_t)\dd{t}+\pdv{g}{x}(t, X_t)\dd{X_t}+\frac 12 \pdv[2]{g}{x}(t, X_t)\cdot (\dd{X_t})^2\]
	where $(\dd{X_t})^2$ is computed according to the rules $\dd{t}\cdot \dd{t} = \dd{t}\cdot \dd{B_t}=0$ and $\dd{B_t}\cdot \dd{B_t}=\dd{t}$.
\end{Theorem}
where an Itô process is defined as follows
\begin{Definition}[Itô Process]
	An Itô process is a stochastic process of the form
	\[X_t = X_a +\int_a^t b(s)\dd{s} + \int_a^t \sigma(s)\dd{B(s)}\]
\end{Definition}
Let us look at an example:
\begin{Example}
	A model of a population is given by the stochastic differential equation
	\[\dv{N_t}{t} = r N_t + \alpha W_t N_t.\]
	Here, $r,\alpha$ are constants and $W_t$ is white noise.
\end{Example}
This is the well known model for a population, except that we have allowed $r$ to vary by a white noise term. The solution in the nonstochastic limit is given by a simple exponential. To solve this, we first rewrite the SDE in standard form:
\[\dd{N_t}=r N_t\dd{t}+\alpha N_t \dd{B_t},\]
or
\[\frac{\dd{N_t}}{N_t} = r \dd{t} + \alpha \dd{B_t}.\]
Inspired by the solution in the deterministic case, we guess $g(t, x)=\ln x$ in Itô's formula, and let $Y_t=g(t, N_t)$. Then, we have
\[\dd{Y_t}=\frac{\dd{N_t}}{N_t}+\frac 12 \alpha^2 \dd{t}.\]
Substituting, we have
\[\dd{Y_t} = \left(r- \frac 12\alpha^2 \right)\dd{t} +\alpha B_t\]
which integrates easily to yield
\[N_t = N_0 \exp\left(\left(r - \frac 12 \alpha^2\right)t+\alpha B_t\right).\]
Clearly, for $\alpha=0$, this reduces to the well known exponential solution. This process is known as \uline{geometric Brownian motion}, which, for example, is a solution to the Black-Scholes equation~\cite{inbook}.

Now, we turn to the questions of existence and uniqueness. Recall the basic theorem for existence and uniqueness of a deterministic differential equation
\begin{Theorem}[Picard-Lindelöf]
	Let $\dot{x} = f(t,x)$ be a differential equation with $f$ defined on a rectangle $[a,b]\times \R^n$. If $f$ is Lipschitz continuous in $x$, with Lipschitz constant independent of time, and continuous in time, then the differential equation has a unique global solution on $[a,b]$.
\end{Theorem}
Note that for uniqueness we do not need the continuity in time; the Lipschitz condition alone suffices. This extends to the stochastic case:
\begin{Theorem}
	Let $\sigma(t,x)$ and $f(t,x)$ be measurable functions on $[a,b]\times \R$ satisfying the Lipschitz condition in $x$, and $\mathcal{F}$ a filtration such that the Brownian motion is adapted to it. Suppose $\xi$ is an $\mathcal{F}_a$-measurable random variable satisfying $\mathbb{E}[\xi^2]<\infty$. Then the stochastic differential equation
	\[\dv{X_t}{t}=f(t, X_t)+\sigma(t, X_t)W_t\]
	has at most one continuous solution on $[a,b]$.
\end{Theorem}
\begin{proof}
	We will not go through the proof in detail. Instead, we will talk about the main steps. Assume we have two solutions $X_t$ and $Y_t$. We seek to estimate $Z_t=X_t-Y_t$
	\begin{enumerate}
		\item We estimate the expectation value $\mathbb{E}(Z_t^2)$, using the Lipschitz condition.
		\item We obtain an integral inequality
			 \[
			 E(Z_t^2) \le 2K^2 (1+b-a)\int_a^t \mathbb{E}(Z_s^2)\dd{s}
			,\]
			which, by the theory of classical differential equations, implies that $Z_t$ is 0 almost surely for all $t$.
		\item Then, we extend the solution to show that $Z_t$ is $0$ almost surely, using sample path continuity.\qedhere
	\end{enumerate}
\end{proof}
The existence theorem is as follows:
\begin{Theorem}
	The stochastic differential equation \eqref{eq:SDEdiff} has a unique solution with initial condition $\xi$, where $\xi^2$ has finite expectation, $\sigma$ and $b$ are Lipschitz in $x$, with Lipschitz constant independent of $t$, and continuous in $t$.
\end{Theorem}
\begin{proof}
	The proof follows by Picard Iteration much as it does in the deterministic case. We define $X_0=\xi$ and
	\[
	X_t^{(n+1)}=X_0+\int_0^t b(s, X_s^{(n)})\dd{s}+\int_0^t \sigma(s, X_s^{(n)})\dd{B_s}
	.\] 
	Then, the proof proceeds in two steps. First, we show that this sequence converges, If it does, and converges in a dominated manner, it satisfies the integral equation by the dominated convergence theorem. 
\end{proof}
It is a known property of initial value problems that the future solution is not dependent on the past. In particular, we can imagine that we have some initial state $x(0)$ and let it evolve a time $t$ to $x(t)$. Then we can let it evolve further. Alternatively, we can consider an initial value problem that has the value $x(t)$ at time $t$. We expect that these two solutions are identical. In a stochastic differential equation, this ``memory'' property is known as the Markov property. 
\begin{Definition}
	A stochastic process $X_t$, with $a\le t \le b$, is said to have the \emph{Markov property} if for all sequences $a < t_1 < \dots < t_n < t < b$ and corresponding $x_1, \dots, x_n$, we have
	\[
	\mathbb{P}(X_t\le x|X_{t_1}=x_1, \dots, X_{t_n}=x_n) = \mathbb{P}(X_t\le x|X_{t_n}=x_n)
	.\] 
\end{Definition}
As an example, all processes with independent increments have the Markov property. The theorem we seek is thus

\begin{Theorem}
	The solution to \eqref{eq:SDEdiff} is a Markov process.
\end{Theorem}

The final property that is of interest to us is time translation invariance. For a deterministic differential equation $\dot{x} = f(x)$, we know that the solution exhibits time translation invariance. In the deterministic case, this is quite easy to see, and follows from the fact that $\dv{t}\varphi(t - t_0) = \varphi'(t-t_0)$. 

In this case, the relevant property is called the \emph{stationary Markov property}
\begin{Definition}
	A stochastic process $X$ is called stationary if the moments are time translation invariant:
	\[
	\langle X_{t_1+\tau}X_{t_2+\tau}\dots X_{t_n+\tau}\rangle = \langle X_{t_1}X_{t_2}\dots X_{t_n}\rangle
	\]
	for all $n,\tau$ and $t_1, \dots, t_n$.
\end{Definition}

Thus, we have our final theorem
\begin{Theorem}
	Suppose that $b(x)$ and $\sigma(x)$ are functions satisfying the Lipschitz condition. Then the solution to
		\begin{equation}\label{eq:SDEdiff2}
		\dv{X_t}{t}=b(X_t)+\sigma(X_t)W_t,
	\end{equation}
	is a stationary Markov process.
\end{Theorem}
As an example of this, we solve the Langevin equation
\begin{Example}
	The Langevin equation is the SDE given by
	\[\dd{X_t} = \mu X_t \dd{t} + \sigma \dd{B_t}.\]
	It has solutions
	\[X_t = e^{\mu t}X_0 + \int_0^t \sigma e^{\mu(t-s)}\dd{B_s}\]
\end{Example}
\begin{proof}
	We multiply by the ``integrating factor'' $e^{-\mu t}$ and consider
	\[Y_t = e^{-\mu t}X_t.\]
	By Itô's formula, we have
	\begin{align*}
	\dd{Y_t} &= -\mu e^{-\mu t}X_t \dd{t} + e^{-\mu t}\dd{X_t}\\
	&=  -\mu e^{-\mu t}X_t \dd{t} + e^{-\mu t}(\mu X_t \dd{t} + \sigma \dd{B_t})\\
	&= e^{-\mu t}\sigma \dd{B_t}
\end{align*}
implying that
\[X_t = e^{\mu t}X_0 + \int_0^t \sigma e^{\mu(t-s)}\dd{B_s}.\qedhere\]
\end{proof}
Finally, we note that some stochastic processes can be described through density functions. Where such a density function is available, it satisfies the \emph{Fokker-Planck Equation}
\begin{Theorem}
	The probability density of the solution to Eq. \eqref{eq:SDEdiff} $p(x,t)$ satisfies the equation
	\[\pdv{p(x,t)}{t}=-\pdv{x}[b(x,t)p(x,t)]+\pdv[2]{x}[D(x,t)p(x,t)]\]
	where $D(x,t) =\frac{\sigma^2(X_t, t)}{2}$ is the diffusion coefficient.
\end{Theorem}
\bibliographystyle{ieeetr}
\bibliography{ref.bib}
\nocite{kuo2005introduction}
\nocite{oksendal2010stochastic}
\end{document}

