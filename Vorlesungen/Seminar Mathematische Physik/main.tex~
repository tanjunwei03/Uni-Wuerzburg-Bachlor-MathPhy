\documentclass[prb,12pt]{revtex4-2}

\usepackage{amsmath, amssymb,physics,amsfonts,amsthm}
\usepackage{enumitem}
\usepackage[most]{tcolorbox}
\usepackage{cancel}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{xurl}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[normalem]{ulem}
\usepackage{transparent}
\usepackage{float}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage[framemethod=TikZ]{mdframed}
\mdfsetup{skipabove=1em,skipbelow=0em, innertopmargin=12pt, innerbottommargin=8pt}
\declaretheoremstyle[headfont=\bfseries\sffamily, bodyfont=\normalfont, mdframed={  } ]{thmbox}
%put nobreak in mdframed
\declaretheorem[style=thmbox, name=Theorem]{Theorem}
\declaretheorem[sibling=Theorem, style=thmbox, name=Definition]{Definition}
\declaretheorem[sibling=Theorem, style=thmbox, name=Corollary]{Corollary}
\declaretheorem[sibling=Theorem, style=thmbox, name=Example]{Example}
\declaretheorem[sibling=Theorem, style=thmbox, name=Proposition]{Proposition}
\declaretheorem[sibling=Theorem, style=thmbox, name=Lemma]{Lemma}
%\newtheorem{Theorem}{Theorem}
%\numberwithin{Theorem}{chapter}
%\newtheorem{Proposition}{Proposition}
%\newtheorem{Lemma}[Theorem]{Lemma}
%\newtheorem{Corollary}[Theorem]{Corollary}
%\newtheorem{Example}[Theorem]{Example}
%\theoremstyle{definition}
\theoremstyle{definition}
\newtheorem{Remark}[Theorem]{Remark}
\theoremstyle{definition}
\newtheorem{Problem}{Problem}
\theoremstyle{definition}
\newenvironment{parts}{\begin{enumerate}[label=(\alph*)]}{\end{enumerate}}
%tikz	
\tcbset{breakable=true,toprule at break = 0mm,bottomrule at break = 0mm}
\usetikzlibrary{patterns}
\usetikzlibrary{matrix}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
% definitions of number sets
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\allowdisplaybreaks
\begin{document}
	\title{Stochastic Differential Equations}
	\author{Jun Wei Tan}
	\email{jun-wei.tan@stud-mail.uni-wuerzburg.de}
	\affiliation{Julius-Maximilians-Universit\"{a}t W\"{u}rzburg}
	\date{\today}
	\maketitle

\begin{Definition}
	A stochastic differential equation is a (formal) equation of the form
	\begin{equation}\label{eq:SDEdiff}
	\dv{X_t}{t}=b(t, X_t)+\sigma(t, X_t)W_t,
\end{equation}
	where $W_t$ is white noise. 
\end{Definition}
The above definition needs some explanation. Intuitively, the motivation is quite clear - we often need to perturb a (deterministic) differential equation with some random noise term. For example, a particle moving in a random environment will be subject to a force that, at each time, is random. As a candidate for white noise, we choose the (also formal) derivative of Brownian motion $W_t = \dv{B_t}{t}$. Physically, this can be justified by thinking of Brownian motion as the ``fundamental'' stochastic process - Brownian motion is the motion generated when the velocity is equal to simple white noise. 

This definition allows is to write down the mathematically rigorous definition of a \emph{solution} to an SDE:
\begin{Definition}
	We say that the stochastic process $X_t$ is a \emph{solution} of the SDE \eqref{eq:SDEdiff} if
	\[X_t = X_0+\int_0^t b(s, X_s)\dd{s}+ \int_0^t \sigma(s, X_s)\dd{B_s}\]
\end{Definition}
At this point, it must be said that there is some ambiguity in this - in particular, we must specify whether the second integral is to be interpreted in the Stratonovich or Itô sense. There are some other interpretations of SDEs proposed in current research \cite{Shi_2012}, but we will also not consider them here. Here, we will consider the Itô integral only.

There are, as usual, two questions when it comes to such differential equations: Firstly, how do we solve them? Secondly, under what conditions do we have existence and uniqueness?

The answer to the first question follows very much like the case of an ordinary differential equation. An ordinary differential equation is integrated most often by the fundamental theorem of calculus. Thus, we turn to the stochastic variation, the Itô formula, for answers.
\begin{Theorem}[The 1-Dimensional Itô Formula]
	Suppose $X_t$ is an Itô process defined by the formula
	\[\dd{X_t}=u\dd{t}+v\dd{B_t}.\]
	Let $g(t,x)\in C^2([0,\infty)\times \R)$. Then
	\[Y_t=g(t, X_t)\]
	is also an Itô process, and
	\[\dd{Y_t}=\pdv{g}{t}(t, X_t)\dd{t}+\pdv{g}{x}(t, X_t)\dd{X_t}+\frac 12 \pdv[2]{g}{x}(t, X_t)\cdot (\dd{X_t})^2\]
	where $(\dd{X_t})^2$ is computed according to the rules $\dd{t}\cdot \dd{t} = \dd{t}\cdot \dd{B_t}=0$ and $\dd{B_t}\cdot \dd{B_t}=\dd{t}$.
\end{Theorem}
This leads us to our first, and most fundamental solution method, both of stochastic and of deterministic equations - guessing the solution. We begin with a simple population model as an example.
\begin{Example}
	A model of a population is given by the stochastic differential equation
	\[\dv{N_t}{t} = r N_t + \alpha W_t N_t.\]
	Here, $r,\alpha$ are constants and $W_t$ is white noise.
\end{Example}
This is the well known model for a population, except that we have allowed $r$ to vary by a white noise term. The solution in the nonstochastic limit is given by a simple exponential. To solve this, we first rewrite the SDE in standard form:
\[\dd{N_t}=r N_t\dd{t}+\alpha N_t \dd{B_t},\]
or
\[\frac{\dd{N_t}}{N_t} = r \dd{t} + \alpha \dd{B_t}.\]
Inspired by the solution in the deterministic case, we guess $g(t, x)=\ln x$ in Ito's formula, and let $Y_t=g(t, N_t)$. Then, we have
\[\dd{Y_t}=\frac{\dd{N_t}}{N_t}+\frac 12 \alpha^2 \dd{t}.\]
Substituting, we have
\[\dd{Y_t} = \left(r- \frac 12\alpha^2 \right)\dd{t} +\alpha B_t\]
which integrates easily to yield
\[N_t = N_0 \exp\left(\left(r - \frac 12 \alpha^2\right)t+\alpha B_t\right).\]
Clearly, for $\alpha=0$, this reduces to the well known exponential solution. This process is known as \uline{geometric Brownian motion}, which, for example, is a solution to the Black-Scholes equation~\cite{inbook}.

Now, we turn to the questions of existence and uniqueness. Recall the basic theorem for existence and uniqueness of a deterministic differential equation
\begin{Theorem}[Picard-Lindelöf]
	Let $\dot{x} = f(t,x)$ be a differential equation with $f$ defined on a rectangle $[a,b]\times \R^n$. If $f$ is locally Lipschitz continuous in $x$, with Lipschitz constant independent of time, and continuous in time, then the differential equation has a unique global solution on $[a,b]$.
\end{Theorem}
We prove the uniqueness similar to in \cite{rudin1976principles}. First, we require an inequality for deterministic differential equations known as Grönwall's inequality:
\begin{Lemma}[Grönwall's Inequality]\label{lemma:Gronwallclassic}
	Suppose $\psi(t)$ satisfies
	\[\psi'(t)\le \beta(t)u(t),\]
	on an interval $[a,b]$. Then $\psi(t)$ is bounded by the solution of the ODE
	\[v'(t)=\beta(t)v(t)\]
	for all time:
	\[\psi'(t)\le \psi(a)\exp\left(\int_a^t \beta(s)\dd{s}\right)\]
\end{Lemma}
\begin{proof}
	We define $v(t)=\exp\left(\int_a^t \beta(s)\dd{s}\right)$. Note that $v$ satisfies the differential equation $v'(t)=\beta(t)v(t)$ and is always strictly greater than $0$. 
	
	Then we consider the derivative of $u(t)/v(t)$:
	\[\dv{t}\frac{u(t)}{v(t)}=\frac{u'(t)v(t)-v'(t)u(t)}{v(t)^2}=\frac{u'(t)-\beta(t)u(t)}{v(t)}\le 0\]
	Thus the function $u(t)/v(t)$ starts at $1$ and is strictly decreasing, which proves the above inequality.
\end{proof}
\begin{Lemma}[Generalized Grönwall's Inequality]
	Statement from \url{https://math.stackexchange.com/questions/4773818/generalized-gronwall-inequality-covering-many-different-applications}:
	
	Suppose $\psi$ is a function on $[a,b]$ that satisfies the differential inequality
	\[\psi'(t)\le F(\psi(t))\]
	with $F\in C^1(\R)$, while $v$ satisfies the differential equation
	\[v'(t)=F(v(t)),\qquad v(a)=\psi(a).\]
	Then $\psi(t) \le v(t)$.
\end{Lemma}
\begin{proof}
	Define $G:\R^2 \to \R$ by
	\[G=\begin{cases}
		\frac{F(x)-F(y)}{x-y} & x\neq y,\\
		F'(x) & x=y.
	\end{cases}\]
	Since $F$ is $C^1$, $G$ is continuous. Then we define $\beta(t)=G(\psi(t), v(t))$ and $w(t)=u(t)-v(t)$. Then
	\[w'(t)=u'(t)-v'(t)\le F(u(t))-F(v(t))=[u(t)-v(t)]G(u(t),v(t))=\beta(t)w(t).\]
	By the classical version of Grönwall's Inequality \ref{lemma:Gronwallclassic}, we have
	\[w(t)\le w(0)\exp\left(\int_a^t \beta(s)\dd{s}\right)=0,\]
	as desired.
\end{proof}
\begin{Corollary}
	Suppose $\psi(t)$ satisfies
	\[\psi'(t) \le \alpha'(t)+\beta(t)\psi(t).\]
	Then
	\[\psi(t)\le \alpha(t)+\int_a^t \alpha(s)\beta(s)\exp\left(\int_s^t \beta(r)\dd{r}\right)\dd{s}\]
\end{Corollary}
\begin{proof}
	We just need to show that the second function is a solution to the differential equation obtained by replacing the inequality with an inequality:
	\begin{align*}
		&\dv{t}\left[\alpha(t) + \int_a^t \alpha(s)\beta(s)\exp\left(\int_s^t \beta(r)\dd{r}\right)\dd{s}\right]\\
		=&\alpha'(t)+\alpha(t)\beta(t)+\int_a^t \alpha(s)\beta(s)\beta(t)\exp\left(\int_s^t \beta(r)\dd{r}\right)\dd{s}\\
		=&\alpha'(t)+\beta(t)\left[\alpha(t) + \int_a^t \alpha(s)\beta(s)\exp\left(\int_s^t \beta(r)\dd{r}\right)\dd{s}\right].\qedhere
	\end{align*}
\end{proof}
Then, we define a slightly weaker notion of Lipschitz continuity:
\begin{Definition}[Linear Growth]
	A function $g(t, x)$ on $[a,b]\times \R$ is said to satisfy the \uline{linear growth condition in $x$} if there exists $K>0$ such that
	\[|g(t, x)|\le K(1+|x|)\]
	for all $t\in [a,b]$ and $x\in \R$.
\end{Definition}
Because of the inequality
\[1+x^2 \le (1+x)^2 \le 2(1+x^2),\]
the earlier definition is equivalent to the following definition:
\begin{Proposition}
	A function $g(t,x)$ satisfies the linear growth condition if there exists $K>0$ such that
	\[|g(t,x)|^2 \le K(1+x^2)\]
\end{Proposition}
The second definition will be more useful in the following proofs. Note that it is indeed stronger
\begin{Proposition}
	Globally Lipschitz continuous functions satisfy the linear growth condition.
\end{Proposition}
\begin{proof}
If $f(x)$ is Lipschitz, we have
\[|f(x) - f(0)| \le K |x|,\]
or
\[|f(x)|\le |f(0)| + K|x|\]
by the reverse triangle inequality.
\end{proof}
Conversely, linear growth does not imply Lipschitz continuity, or any kind of continuity
\begin{Example}
	Let
	\[f(x) = \begin{cases}
		x & x\in \Q,\\
		0 & \text{otherwise}.
	\end{cases}\]
	Clearly, $f$ grows linearly, but is not Lipschitz continuous, or even continuous at any point.
\end{Example}


Now, we continue to the proof of uniqueness:
\begin{Theorem}
	Let $\sigma(t,x)$ and $f(t,x)$ be measurable functions on $[a,b]\times \R$ satisfying the Lipschitz condition in $x$. Suppose $\xi$ is an $\mathcal{F}_a$-measurable random variable satisfying $\mathbb{E}[\xi^2]<\infty$. Then the stochastic differential equation
	\[\dv{X_t}{t}=f(t, X_t)+\sigma(t, X_t)W_t\]
	has at most one solution on $[a,b]$.
\end{Theorem}
\begin{proof}
	As one might expect, we assume that we have two solutions $X_t$ and $Y_t$ to the stochastic differential equation. Then $Z_t=X_t-Y_t$ is a stochastic process satisfying the stochastic differential equation, or equivalently integral equation
	\[Z_t = \int_a^t (\sigma(s, X_s) - \sigma (s, Y_s))\dd{B(s)}+\int_a^t (f(s, X_s) - f(s, Y_s))\dd{s}.\]
	Then we consider the expectation value of the square $Z_t^2$. Because $(a+b)^2 < 2(a^2+b^2)$, we have
	\[Z_t^2 \le 2\left[\left(\int_a^t (\sigma(s, X_s) - \sigma(s, Y_s))\dd{B_s}\right)^2 + \left(\int_a^t (f(s, X_s) - f(s, Y_s))\dd{s}\right)^2\right].\]
	Now, we aim to estimate the expectation value of the two quantities in parentheses. As with deterministic differential equations, this is done using the Lipschitz condition:
	\begin{align*}
		E\left(\int_a^t (\sigma(s, X_s) - \sigma(s, Y_s))\dd{B(s)}\right)^2&= \int_a^t E[(\sigma(s, X_s)-\sigma(s, Y_s))^2]\dd{s}\\
		&\le K^2 \int_a^t E(Z_s^2)\dd{s}
	\end{align*} 
where the first line comes from the Ito isometry and the second comes from the Lipschitz condition. Similarly, for the second term, we have
\begin{align*}
	\left(\int_a^t (f(s, X_s) - f(s, Y_s))\dd{s}\right)^2 &\le (t-a)\int_a^t (f(s, X_s) - f(s, Y_s))^2 \dd{s}\\
	&\le (b-a)K^2 \int_a^t Z_s^2\dd{s}.
\end{align*}
Putting these together, we get
\[E(Z_t^2)\le 2K^2(1+b-a)\int_a^t E(Z_s^2)\dd{s}.\]
This is a classical differential equation, and it is well known that the result is $E(Z_t^2)=0$ for all $t\in [a,b]$.

This shows that $Z_t=0$, or $X_t=Y_t$ almost surely for all $t\in [a,b]$. 

The last step is to extend this to show that $X_t=Y_t$ almost surely.

To do so, first we enumerate the rationals in $[a,b]$ as $r_1,r_2, \dots$. Because $Z_t=0$ almost surely for each $t$, we can find $\Omega_n$ with $\mathbb{P}(\Omega_n)=1$ and $Z_{r_n}=0$ on $\Omega_n$. Let $\Omega' = \bigcap_{k=1}^\infty \Omega_k$. Then $\mathbb{P}(\Omega')=1$.

Then, by (almost surely path) continuity, we choose $\Omega^{\prime\prime}$ such that $\mathbb{P}(\Omega^{\prime\prime})=1$ and $t\mapsto Z_t(\omega)$ is continuous for all $\omega\in \Omega^{\prime\prime}$.

Then, we let $\Omega_0=\Omega'\cap \Omega^{\prime\prime}$. Clearly, $Z_T$ is a continuous function that vanishes at all rationals in $[a,b]$. Thus, $Z_t$ is $0$ for all $t\in [a,b]$ and $\omega\in \Omega_0$.
\end{proof}
Note that the last part of the proof is necessary. The first part, which emulated the classical case, showed that the two processes are the same for all time almost certainly. However, unlike the classical case, knowing the distributions for all time $t$ does not suffice as a solution. As an example,
\begin{Example}
	Consider the two stochastic processes
	\[
	X_t(\omega) = \begin{cases}
		1 & t = \omega\\
		0  & \text{otherwise,}
	\end{cases}
	\]
	and
	\[
	Y_t=0
	.\] 
	Then it is clear that for all $t\in [0,\infty)$ we have $X_t=Y_t$ almost certainly, but these two processes are clearly different. For example, $X_t$ is sample path discontinuous, while $Y_t$ is.
\end{Example}
Now that we have proven the weaker result (only of uniqueness), we can move on to prove the result of existence and uniqueness. It is instructive to first consider the proof for a constant $\sigma$ in Eq.~\eqref{eq:SDEdiff}
\begin{Theorem}
	The differential equation \eqref{eq:SDEdiff} has a solution with $\sigma$ constant, if $b(t, x)$ is Lipschitz continous in $x$.
\end{Theorem}
\begin{proof}
	Uniqueness follows from the previous theorem. For existence, we iterate
	\[
	X_0(t, \omega)=\omega,\qquad X_{n+1}(t, \omega)=\omega+\int_0^t b(s, X_n(s, \omega))\dd{s}+\sigma B(t)
	.\] 
	By hypothesis, $b$ is continuous. Thus, we can show by induction that $X_n$ has continuous paths --- if we suppose so, then we can find $\Omega$ such that $\mathbb{P}(\Omega)=1$ and $t\mapsto X_n(t, \omega)$ is continuous for all $\omega\in \Omega$. 

	For all such $\omega$, $\int_0^t b(X_n(s, \omega))\dd{s}$ is continuous as a function of $t$ by the fundamental theorem of calculus, and $B(t)$ is known to be continuous. Thus, all the $X_n$s are almost certainly sample path continuous.

	Then, we must show that the limit process satisfies the integral equation. To do so, first we show uniform convergence on compact intervals. This follows as it does in a deterministic differential equation: Since $b$ is Lipschitz, we can find $B$ such that
	\[
	|X_{n+1}(t, \omega)-X_n(t, \omega)| \le B \int_0^t |X_n(s, \omega) - X_{n-1}(s, \omega)|\dd{s} 
	.\] 
	Then, we proceed by induction: Since $|X_0|\le \omega$, we can show that
	\[
	|X_n(t)| \le \frac{C \omega B^n t^n}{n!}
	\]
	for some $C>0$. Thus, $X(t, \omega)$ converges uniformly for all $t\in [0,T]$ for all $T>0$. 

	Then, by the dominated convergence theorem, we have integrability with respect to $t$, as well as measurability with respect to $\omega$. The dominated convergence theorem also shows that the limit function satisfies the integral equation.
\end{proof}
Now, we are ready to handle the general case
\begin{Theorem}
	The stochastic differential equation \eqref{eq:SDEdiff} has a unique solution where $\sigma$ and $b$ are uniformly Lipschitz in $x$.
\end{Theorem}
\begin{proof}
	Unlike in the previous theorem, we cannot do this by estimating the solutions directly. Instead, we apply our estimates to 2nd moments. For brevity, we assume that both functions are Lipschitz with the same Lipschitz constant; taking the maximum of the constants yields this result. 
	\begin{align*}
		&\mathbb{E}|X_{n+1}(t) - X_n(t)|\\
	\le &\mathbb{E}\left( \int_0^t (b(s, X^{(n)}_s) - b(s, X^{(n)}_s)) + \int_0^t \sigma(s, X^{(n)}_s) - \sigma(s, X^{(n-1)}_s)\dd{s}\right)^2\\
		\le & 2\mathbb{E}\left( \int_0^t (b(s, X^{(n)}_s) - b(s, X^{(n-1)}_s))\dd{s} \right)^2 + 2\mathbb{E}\left( \int_0^t (\sigma(s, X^{(n)}_s) - \sigma(s, X^{(n-1)}_s)) \right)^2\\
		\le& 2B^2 \mathbb{E}\left( \int_0^t |X^{(n)}_s - X^{(n-1)}_s|\dd{s} \right)^2 + 2B^2 \int_0^t \mathbb{E}|X^{(n)}_s - X^{(n-1)}_s|^2 \dd{s}\\
		\le& 2B^2 \mathbb{E}\left( t\int_0^t |X^{(n)}_s - X^{(n-1)}_s|^2\dd{s} \right)+2B^2 \int_0^t \mathbb{E}|X^{(n)}_s - X^{(n-1)}_s|^2\dd{s}\\
		\le& 2B^2(T+1)\int_0^t \mathbb{E}|X^{(n)}_s - X^{(n-1)}_s|^2\dd{s}
	\end{align*}
	for all $t\in [0,T]$, where we have used the inequality $(x+y)^2 \le 2x^2+2y^2$ in the first line, the Ito isometry in the second, and the Cauchy Schwarz inequality in the third.

	Now, we are basically done. Since $X^{(n)}_t \to X_t$ uniformly, Lipschitz continuity implies that $\mu(X^{(n)}_t) \to \mu(X_t)$ and $b(X^{(n)}_t)\to b(X_t)$ almost surely (since their squared expectations vanish). 

	By the Ito isometry, we then have
	\[
	\lim_{n \to \infty} \int_0^t \sigma(s, X^{(n)}_s)\dd{B(s)} = \int_0^t \sigma(s, X(s))\dd{B(s)}
	\]
	and
	\[
	\lim_{n \to \infty} \int_0^t b(s, X^{(n)}_s)\dd{s} = \int_0^t b(X_s)\dd{s}
	.\] 
\end{proof}
\bibliographystyle{ieeetr}
\bibliography{ref.bib}
\end{document}

